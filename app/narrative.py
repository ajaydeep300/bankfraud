import os
import base64
import json
import time
from openai import OpenAI
from dotenv import load_dotenv
import re
import argparse

def load_environment_variables():
    load_dotenv()

def initialize_openai_client():
    return OpenAI(api_key=os.environ["OPENAI_API_KEY"])

def create_assistant(client):
    return client.beta.assistants.create(
        name="Financial Analyst Assistant",
        instructions="You are an expert financial analyst. Use your knowledge base to answer questions about audited financial statements.",
        model="gpt-4o-mini",
        tools=[{"type": "file_search"}]
    )

def create_vector_store(client):
    return client.beta.vector_stores.create(name="Financial Statements")

def upload_files_to_vector_store(client, vector_store, file_paths):
    file_streams = [open(path, "rb") for path in file_paths]
    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(
        vector_store_id=vector_store.id,
        files=file_streams
    )
    for stream in file_streams:
        stream.close()
    return file_batch

def update_assistant_with_vector_store(client, assistant, vector_store):
    client.beta.assistants.update(
        assistant_id=assistant.id,
        tool_resources={"file_search": {"vector_store_ids": [vector_store.id]}}
    )

def create_thread(client, assistant):
    return client.beta.threads.create(
        messages=[
            {
                "role": "user",
                "content": (
                    "We are going to analyze the accounts, transactions, and fraud information of a user to determine "
                    "if the user has committed a financial crime. There will be an assistant and a user; the user will "
                    "ask questions from the narrative questions list. The assistant will interpret these questions, look "
                    "for supporting data in the uploaded files, and build an answer using that data."
                )
            }
        ]
    )

def run_assistant(client, assistant_id, thread_id, user_instructions):
    run = client.beta.threads.runs.create_and_poll(
        thread_id=thread_id,
        assistant_id=assistant_id,
        instructions=user_instructions
    )
    return wait_on_run(client, run, thread_id)

def wait_on_run(client, run, thread_id):
    while run.status in ["queued", "in_progress"]:
        time.sleep(0.5)
        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)
    return run

def process_questions(client, assistant, thread, patterns):
    for ques in patterns:
        # Post the question to the thread
        client.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=ques
        )

        # Ask the assistant to answer, referencing the file search data
        run = run_assistant(
            client,
            assistant.id,
            thread.id,
            (
                "Identify yourself as a financial statement analyst. The files contain analysis about a bank customer's "
                "transactions. We have to build a final narrative that concludes whether a financial crime has been committed. "
                "Answer the question by referencing the file search data where relevant."
            )
        )

        # Retrieve all messages in this run
        messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))
        message_content = messages[0].content[0].text  # The assistant's response
        annotations = message_content.annotations
        citations = []

        # Example: replace annotation text with bracket references, build citations list
        for index, annotation in enumerate(annotations):
            message_content.value = message_content.value.replace(annotation.text, f"[{index}]")
            if file_citation := getattr(annotation, "file_citation", None):
                cited_file = client.files.retrieve(file_citation.file_id)
                citations.append(f"[{index}] {cited_file.filename}")

        print(message_content.value)
        print("\n".join(citations))

########################################################################
# New: Wrap entire logic in a function that can also accept the analysis file
########################################################################
def run_narrative_workflow(file_paths, analysis_file=None):
    """
    Runs the entire narrative logic, optionally including an analysis file
    generated by the previous script (e.g., 'files/responses_log.txt').
    """

    # If the analysis file is provided, include it along with the other files
    all_files = list(file_paths)
    if analysis_file and os.path.isfile(analysis_file):
        all_files.append(analysis_file)

    load_environment_variables()
    client = initialize_openai_client()
    assistant = create_assistant(client)
    vector_store = create_vector_store(client)

    # Upload all files (original + analysis) to vector store
    file_batch = upload_files_to_vector_store(client, vector_store, all_files)
    print("File batch status:", file_batch.status)
    print("File batch counts:", file_batch.file_counts)

    update_assistant_with_vector_store(client, assistant, vector_store)

    thread = create_thread(client, assistant)
    print("Thread resources:", thread.tool_resources.file_search)

    # Your original "patterns" or narrative questions
    patterns = [
        "What rule(s) did the alert generate for?",
        "What is the Card/Account Program?",
        "What is the Card/Account program description?",
        "What is the primary cause(s) for the alerting activity?",
        "What are other major credit(s) to the account?",
        "What are other major debit(s) to the account?",
        "What is the overall flow of funds through this account?",
        "Is activity occurring in/near Cardholder's residing area?",
        "Is the expected activity showing a spike or fairly consistent?",
        "Are there any additional red flags to address and can they be mitigated?",
        "Should this alert be escalated or closed as non-issue?",
    ]

    process_questions(client, assistant, thread, patterns)

############################
# Keep your original main
############################
def cli_main():
    parser = argparse.ArgumentParser(description="Run narrative analysis with provided files.")
    parser.add_argument("files", nargs="+", help="Paths to the files to be uploaded.")
    parser.add_argument(
        "--analysis", 
        dest="analysis_file",
        help="Path to the analysis file generated by the previous script (optional)."
    )
    args = parser.parse_args()

    run_narrative_workflow(args.files, analysis_file=args.analysis_file)

if __name__ == "__main__":
    cli_main()
